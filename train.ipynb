{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# imports\n",
    "import torch as t, torch.nn as nn, torch.nn.functional as tnnF, torch.distributions as tdist\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision as tv, torchvision.transforms as tr\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import wideresnet # from The Google Research Authors\n",
    "import json"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Sampling\n",
    "#from tqdm import tqdm\n",
    "t.backends.cudnn.benchmark = True\n",
    "t.backends.cudnn.enabled = True\n",
    "seed = 1\n",
    "\n",
    "# images RGB 32x32\n",
    "im_sz = 32"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# get random subset of data\n",
    "class DataSubset(Dataset):\n",
    "    def __init__(self, base_dataset, inds=None, size=-1):\n",
    "        self.base_dataset = base_dataset\n",
    "        if inds is None:\n",
    "            inds = np.random.choice(list(range(len(base_dataset))), size, replace=False)\n",
    "        self.inds = inds\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        base_ind = self.inds[index]\n",
    "        return self.base_dataset[base_ind]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inds)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# setup Wide_ResNet\n",
    "# Uses The Google Research Authors, file wideresnet.py\n",
    "class F(nn.Module):\n",
    "    def __init__(self, depth=28, width=2, norm=None, dropout_rate=0.0, n_classes=10):\n",
    "        super(F, self).__init__()\n",
    "        self.f = wideresnet.Wide_ResNet(depth, width, norm=norm, dropout_rate=dropout_rate)\n",
    "        self.energy_output = nn.Linear(self.f.last_dim, 1)\n",
    "        self.class_output = nn.Linear(self.f.last_dim, n_classes)\n",
    "\n",
    "    def classify(self, x):\n",
    "        penult_z = self.f(x)\n",
    "        return self.class_output(penult_z).squeeze()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# various utilities\n",
    "def cycle(loader):\n",
    "    while True:\n",
    "        for data in loader:\n",
    "            yield data"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Load in chosen dataset cifar10\n",
    "def get_data(args):\n",
    "    transform_train = tr.Compose(\n",
    "        [tr.Pad(4, padding_mode=\"reflect\"),\n",
    "         tr.RandomCrop(im_sz),\n",
    "         tr.RandomHorizontalFlip(),\n",
    "         tr.ToTensor(),\n",
    "         tr.Normalize((.5, .5, .5), (.5, .5, .5)),\n",
    "         lambda x: x + args.sigma * t.randn_like(x)]\n",
    "    )\n",
    "    transform_test = tr.Compose(\n",
    "        [tr.ToTensor(),\n",
    "         tr.Normalize((.5, .5, .5), (.5, .5, .5)),\n",
    "         lambda x: x + args.sigma * t.randn_like(x)]\n",
    "    )\n",
    "    def dataset_fn(train, transform):\n",
    "        return tv.datasets.CIFAR10(root=args.data_root, transform=transform, download=True, train=train)\n",
    "\n",
    "    # get all training inds\n",
    "    full_train = dataset_fn(True, transform_train)\n",
    "    all_inds = list(range(len(full_train)))\n",
    "    # set seed\n",
    "    np.random.seed(1234)\n",
    "    # shuffle\n",
    "    np.random.shuffle(all_inds)\n",
    "    # seperate out validation set\n",
    "    if args.n_valid is not None:\n",
    "        valid_inds, train_inds = all_inds[:args.n_valid], all_inds[args.n_valid:]\n",
    "    else:\n",
    "        valid_inds, train_inds = [], all_inds\n",
    "    train_inds = np.array(train_inds)\n",
    "    train_labeled_inds = []\n",
    "    other_inds = []\n",
    "    train_labels = np.array([full_train[ind][1] for ind in train_inds])\n",
    "    if args.labels_per_class > 0:\n",
    "        for i in range(args.n_classes):\n",
    "            print(i)\n",
    "            train_labeled_inds.extend(train_inds[train_labels == i][:args.labels_per_class])\n",
    "            other_inds.extend(train_inds[train_labels == i][args.labels_per_class:])\n",
    "    else:\n",
    "        train_labeled_inds = train_inds\n",
    "\n",
    "    dset_train = DataSubset(\n",
    "        dataset_fn(True, transform_train),\n",
    "        inds=train_inds)\n",
    "    dset_train_labeled = DataSubset(\n",
    "        dataset_fn(True, transform_train),\n",
    "        inds=train_labeled_inds)\n",
    "    dset_valid = DataSubset(\n",
    "        dataset_fn(True, transform_test),\n",
    "        inds=valid_inds)\n",
    "    dload_train = DataLoader(dset_train, batch_size=args.batch_size, shuffle=True, num_workers=4, drop_last=True)\n",
    "    dload_train_labeled = DataLoader(dset_train_labeled, batch_size=args.batch_size, shuffle=True, num_workers=4, drop_last=True)\n",
    "    dload_train_labeled = cycle(dload_train_labeled)\n",
    "    dset_test = dataset_fn(False, transform_test)\n",
    "    dload_valid = DataLoader(dset_valid, batch_size=100, shuffle=False, num_workers=4, drop_last=False)\n",
    "    dload_test = DataLoader(dset_test, batch_size=100, shuffle=False, num_workers=4, drop_last=False)\n",
    "    return dload_train, dload_train_labeled, dload_valid,dload_test"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# calculate loss and accuracy for periodic printout\n",
    "def eval_classification(f, dload, device):\n",
    "    corrects, losses = [], []\n",
    "    for x_p_d, y_p_d in dload:\n",
    "        x_p_d, y_p_d = x_p_d.to(device), y_p_d.to(device)\n",
    "        logits = f.classify(x_p_d)\n",
    "        loss = nn.CrossEntropyLoss(reduce=False)(logits, y_p_d).cpu().numpy()\n",
    "        losses.extend(loss)\n",
    "        correct = (logits.max(1)[1] == y_p_d).float().cpu().numpy()\n",
    "        corrects.extend(correct)\n",
    "    loss = np.mean(losses)\n",
    "    correct = np.mean(corrects)\n",
    "    return correct, loss"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# save checkpoint data\n",
    "def checkpoint(f, opt, epoch_no, tag, args, device):\n",
    "    f.cpu()\n",
    "    ckpt_dict = {\n",
    "        \"model_state_dict\": f.state_dict(),\n",
    "        'optimizer_state_dict': opt.state_dict(),\n",
    "        'epoch': epoch_no,\n",
    "        #\"replay_buffer\": buffer\n",
    "    }\n",
    "    t.save(ckpt_dict, os.path.join(args.save_dir, tag))\n",
    "    f.to(device)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# main function for training\n",
    "# Uses args from class below\n",
    "def main(args):\n",
    "    if not os.path.exists(args.save_dir):\n",
    "        os.makedirs(args.save_dir)\n",
    "\n",
    "    with open(f'{args.save_dir}/params.txt', 'w') as f:\n",
    "        json.dump(args.__dict__, f)\n",
    "    if args.print_to_log:\n",
    "        sys.stdout = open(f'{args.save_dir}/log.txt', 'w')\n",
    "\n",
    "    t.manual_seed(seed)\n",
    "    if t.cuda.is_available():\n",
    "        t.cuda.manual_seed_all(seed)\n",
    "\n",
    "    # datasets\n",
    "    dload_train, dload_train_labeled, dload_valid, dload_test = get_data(args)\n",
    "\n",
    "    device = t.device('cuda' if t.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # setup Wide_ResNet\n",
    "    f = F(args.depth, args.width, args.norm, dropout_rate=args.dropout_rate, n_classes=args.n_classes)\n",
    "    \n",
    "    # push to GPU\n",
    "    f = f.to(device)\n",
    "\n",
    "    # optimizer\n",
    "    params = f.class_output.parameters() if args.clf_only else f.parameters()\n",
    "    optim = t.optim.Adam(params, lr=args.lr, betas=[.9, .999], weight_decay=args.weight_decay)\n",
    "\n",
    "    # epoch_start\n",
    "    epoch_start = 0\n",
    "    \n",
    "    # load checkpoint?\n",
    "    if args.load_path:\n",
    "        print(f\"loading model from {args.load_path}\")\n",
    "        ckpt_dict = t.load(args.load_path)\n",
    "        f.load_state_dict(ckpt_dict[\"model_state_dict\"])\n",
    "        optim.load_state_dict(ckpt_dict['optimizer_state_dict'])\n",
    "        epoch_start = ckpt_dict['epoch']\n",
    "\n",
    "    # push to GPU\n",
    "    f = f.to(device)\n",
    "    \n",
    "    # Show train set loss/accuracy after reload\n",
    "    f.eval()\n",
    "    with t.no_grad():\n",
    "        correct, loss = eval_classification(f, dload_train, device)\n",
    "        print(\"Epoch {}: Train Loss {}, Train Acc {}\".format(epoch_start, loss, correct))\n",
    "    f.train()\n",
    "\n",
    "    best_valid_acc = 0.0\n",
    "    cur_iter = 0\n",
    "    \n",
    "    # loop over epochs\n",
    "    for epoch in range(epoch_start, epoch_start + args.n_epochs):\n",
    "        # loop over data in batches\n",
    "        # x_p_d sample from dataset\n",
    "        for i, (x_p_d, _) in enumerate(dload_train): #tqdm(enumerate(dload_train)):\n",
    "\n",
    "            #print(\"x_p_d_shape\",x_p_d.shape)\n",
    "            x_p_d = x_p_d.to(device)\n",
    "            x_lab, y_lab = dload_train_labeled.__next__()\n",
    "            x_lab, y_lab = x_lab.to(device), y_lab.to(device)\n",
    "\n",
    "            # initialize loss\n",
    "            L = 0.\n",
    "            \n",
    "            # normal cross entropy loss function\n",
    "            # maximize log p(y | x)\n",
    "            logits = f.classify(x_lab)\n",
    "            l_p_y_given_x = nn.CrossEntropyLoss()(logits, y_lab)\n",
    "            if cur_iter % args.print_every == 0:\n",
    "                acc = (logits.max(1)[1] == y_lab).float().mean()\n",
    "                print('P(y|x) {}:{:>d} loss={:>14.9f}, acc={:>14.9f}'.format(epoch,\n",
    "                                                                             cur_iter,\n",
    "                                                                             l_p_y_given_x.item(),\n",
    "                                                                             acc.item()))\n",
    "            # add to loss\n",
    "            L += l_p_y_given_x\n",
    "\n",
    "            # break if the loss diverged\n",
    "            if L.abs().item() > 1e8:\n",
    "                print(\"Divergwence error\")\n",
    "                1/0\n",
    "\n",
    "            # Optimize network using our loss function L\n",
    "            optim.zero_grad()\n",
    "            L.backward()\n",
    "            optim.step()\n",
    "            cur_iter += 1\n",
    "\n",
    "        # do checkpointing\n",
    "        if epoch % args.ckpt_every == 0:\n",
    "            checkpoint(f, optim, epoch, f'ckpt_{epoch}.pt', args, device)\n",
    "\n",
    "        # Print performance assesment \n",
    "        if epoch % args.eval_every == 0:\n",
    "            f.eval()\n",
    "            with t.no_grad():\n",
    "                # train set\n",
    "                correct, loss = eval_classification(f, dload_train, device)\n",
    "                print(\"Epoch {}: Train Loss {}, Train Acc {}\".format(epoch, loss, correct))\n",
    "\n",
    "                # test set\n",
    "                correct, loss = eval_classification(f, dload_test, device)\n",
    "                print(\"Epoch {}: Test Loss {}, Test Acc {}\".format(epoch, loss, correct))\n",
    "\n",
    "                # validation set\n",
    "                correct, loss = eval_classification(f, dload_valid, device)\n",
    "                print(\"Epoch {}: Valid Loss {}, Valid Acc {}\".format(epoch, loss, correct))\n",
    "\n",
    "            f.train()\n",
    "            \n",
    "        # do \"last\" checkpoint\n",
    "        checkpoint(f, optim, epoch, \"last_ckpt.pt\", args, device)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Setup parameters\n",
    "# defaults for paper\n",
    "# --lr .0001 --dataset cifar10 --optimizer adam --p_x_weight 1.0 --p_y_given_x_weight 1.0 \n",
    "# --p_x_y_weight 0.0 --sigma .03 --width 10 --depth 28 --save_dir /YOUR/SAVE/DIR \n",
    "# --plot_uncond --warmup_iters 1000\n",
    "#\n",
    "class train_args():\n",
    "    def __init__(self, param_dict):\n",
    "        # set defaults\n",
    "        self.dataset = \"cifar10\"\n",
    "        self.n_classes = 10\n",
    "        self.width = 10 # wide-resnet widen_factor\n",
    "        self.depth = 28  # wide-resnet depth\n",
    "        self.sigma = .03 # image transformation\n",
    "        self.data_root = \"../dataset\" \n",
    "        # optimization\n",
    "        self.lr = 1e-4\n",
    "        self.clf_only = False #action=\"store_true\", help=\"If set, then only train the classifier\")\n",
    "        self.labels_per_class = -1# help=\"number of labeled examples per class, if zero then use all labels\")\n",
    "        self.batch_size = 64\n",
    "        self.n_epochs = 1\n",
    "        # regularization\n",
    "        self.dropout_rate = 0.0\n",
    "        self.sigma = 3e-2 # help=\"stddev of gaussian noise to add to input, .03 works but .1 is more stable\")\n",
    "        self.weight_decay = 0.0\n",
    "        # network\n",
    "        self.norm = None # choices=[None, \"norm\", \"batch\", \"instance\", \"layer\", \"act\"], help=\"norm to add to weights, none works fine\")\n",
    "        # logging + evaluation\n",
    "        self.save_dir = './experiment'\n",
    "        self.ckpt_every = 10 # help=\"Epochs between checkpoint save\")\n",
    "        self.eval_every = 1 # help=\"Epochs between evaluation\")\n",
    "        self.print_every = 100 # help=\"Iterations between print\")\n",
    "        self.load_path = None # path for checkpoint to load\n",
    "        self.print_to_log = False #\", action=\"store_true\", help=\"If true, directs std-out to log file\")\n",
    "        self.n_valid = 5000 # number of validation images\n",
    "        \n",
    "        # set from inline dict\n",
    "        for key in param_dict:\n",
    "            #print(key, '->', param_dict[key])\n",
    "            setattr(self, key, param_dict[key])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# setup change from defaults\n",
    "# For paper defaults\n",
    "# Added best_valid_ckpt load\n",
    "inline_parms = {\"lr\": .0001} #, \"load_path\": './all1/last_ckpt.pt'} \n",
    "\n",
    "# instantiate\n",
    "args = train_args(inline_parms)\n",
    "\n",
    "# run\n",
    "main(args)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}